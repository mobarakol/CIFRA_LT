{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImbalancedDatasetSampler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbW5W+WFQUfefqVMPbuCHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/CIFRA_LT/blob/main/ImbalancedDatasetSampler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4RNdBTIPzGq"
      },
      "source": [
        "# Imbalanced Dataset Sampler\n",
        "(reference: https://github.com/ufoym/imbalanced-dataset-sampler) <br>\n",
        "To solve imbalanced classes problem, a widely adopted technique is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling). Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/2270240/40656410-e0baa230-6376-11e8-8904-c092fb38fcdc.png\" alt=\"d\">\n",
        "\n",
        "\n",
        "In this imbalanced dataset sampler method, \n",
        "\n",
        "- rebalance the class distributions when sampling from the imbalanced dataset <br>\n",
        "- estimate the sampling weights automatically <br>\n",
        "- avoid creating a new balanced dataset <br>\n",
        "- mitigate overfitting when it is used in conjunction with data augmentation techniques <br>\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/2270240/40677251-b08f504a-63af-11e8-9653-f28e973a5664.png\" alt=\"d\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FntlSoc4P5OG"
      },
      "source": [
        "Basic Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PSBGd_f3uul"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='BalancedLSF Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=50, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=2048, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=100, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "parser.add_argument('--imb_factor', default=0.01, type=float, help='Imbalanced factor')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfX_1EJOP_Oj"
      },
      "source": [
        "CIFAR imbalanced Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM7zy24SHgsk"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "class IMBALANCECIFAR10(torchvision.datasets.CIFAR10):\n",
        "    cls_num = 10\n",
        "\n",
        "    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)\n",
        "        np.random.seed(rand_number)\n",
        "        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n",
        "        self.gen_imbalanced_data(img_num_list)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.targets\n",
        "\n",
        "    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n",
        "        img_max = len(self.data) / cls_num\n",
        "        img_num_per_cls = []\n",
        "        if imb_type == 'exp':\n",
        "            for cls_idx in range(cls_num):\n",
        "                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
        "                img_num_per_cls.append(int(num))\n",
        "        elif imb_type == 'step':\n",
        "            for cls_idx in range(cls_num // 2):\n",
        "                img_num_per_cls.append(int(img_max))\n",
        "            for cls_idx in range(cls_num // 2):\n",
        "                img_num_per_cls.append(int(img_max * imb_factor))\n",
        "        else:\n",
        "            img_num_per_cls.extend([int(img_max)] * cls_num)\n",
        "        return img_num_per_cls\n",
        "\n",
        "    def gen_imbalanced_data(self, img_num_per_cls):\n",
        "        new_data = []\n",
        "        new_targets = []\n",
        "        targets_np = np.array(self.targets, dtype=np.int64)\n",
        "        classes = np.unique(targets_np)\n",
        "        self.num_per_cls_dict = dict()\n",
        "        for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
        "            self.num_per_cls_dict[the_class] = the_img_num\n",
        "            idx = np.where(targets_np == the_class)[0]\n",
        "            np.random.shuffle(idx)\n",
        "            selec_idx = idx[:the_img_num]\n",
        "            new_data.append(self.data[selec_idx, ...])\n",
        "            new_targets.extend([the_class, ] * the_img_num)\n",
        "        new_data = np.vstack(new_data)\n",
        "        self.data = new_data\n",
        "        self.targets = new_targets\n",
        "        \n",
        "    def get_cls_num_list(self):\n",
        "        cls_num_list = []\n",
        "        for i in range(self.cls_num):\n",
        "            cls_num_list.append(self.num_per_cls_dict[i])\n",
        "        return cls_num_list\n",
        "\n",
        "class IMBALANCECIFAR100(IMBALANCECIFAR10):\n",
        "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    This is a subclass of the `CIFAR10` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "    filename = \"cifar-100-python.tar.gz\"\n",
        "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
        "    train_list = [\n",
        "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
        "    ]\n",
        "    meta = {\n",
        "        'filename': 'meta',\n",
        "        'key': 'fine_label_names',\n",
        "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
        "    }\n",
        "    cls_num = 100"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xj89lHHQIQ-"
      },
      "source": [
        "Installing Imbalanced Dataset Sampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXmStSLaIU4P",
        "outputId": "185063c0-76ad-4e28-a2c0-6faeef4804c7"
      },
      "source": [
        "!pip install https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip\n",
            "  Downloading https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip\n",
            "\u001b[K     - 297 kB 1.7 MB/s\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from torchsampler==0.1.1) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/dist-packages (from torchsampler==0.1.1) (0.11.1+cu111)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsampler==0.1.1) (1.1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->torchsampler==0.1.1) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->torchsampler==0.1.1) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->torchsampler==0.1.1) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsampler==0.1.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsampler==0.1.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsampler==0.1.1) (1.15.0)\n",
            "Building wheels for collected packages: torchsampler\n",
            "  Building wheel for torchsampler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchsampler: filename=torchsampler-0.1.1-py3-none-any.whl size=3839 sha256=0670102e5a82bfd7d5ea5620ae6e838db45181bb5a4b1a6a5066dc8891fc30f6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b9uofjta/wheels/52/7b/7d/ce0e0ddbb7864877a0e31a96f883a928791ebfa6eaf7b52f87\n",
            "Successfully built torchsampler\n",
            "Installing collected packages: torchsampler\n",
            "Successfully installed torchsampler-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nTtgbonQOrZ"
      },
      "source": [
        "Comparing Imbalanced Dataset Sampler over pytorch IID sampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_FWdg9vG-ae",
        "outputId": "342e7de5-cbd6-42f4-90ce-1a8e9b50a6aa"
      },
      "source": [
        "from torchsampler import ImbalancedDatasetSampler\n",
        "seed_everything()\n",
        "mean_cifar10, std_cifar10 = (0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)\n",
        "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "            transforms.Normalize(mean_cifar10, std_cifar10), ])\n",
        "transform_test = transforms.Compose([transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_cifar10, std_cifar10),])\n",
        "#train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_dataset = IMBALANCECIFAR10(root='./data', imb_type='exp', imb_factor=args.imb_factor, rand_number=0, train=True, download=True, transform=transform_train)\n",
        "print('Class frequency in Train Dataset',train_dataset.get_cls_num_list())\n",
        "\n",
        "#Standard IID Batch Sampler\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,num_workers=4)\n",
        "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    cls_batch_sampler = [(targets==i).sum().item() for i in range(10)]\n",
        "    print('Standard IID Batch Sampler:',cls_batch_sampler)\n",
        "    break\n",
        "\n",
        "#Imbalanced Dataset Sampler with library\n",
        "train_loader_bal = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, num_workers=4, sampler=ImbalancedDatasetSampler(train_dataset))\n",
        "for batch_idx, (inputs, targets) in enumerate(train_loader_bal):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    cls_batch_sampler = [(targets==i).sum().item() for i in range(10)]\n",
        "    print('Imbalanced Dataset Sampler with Lib:',cls_batch_sampler)\n",
        "    break\n",
        "\n",
        "\n",
        "#Imbalanced Dataset Sampler with Pytorch sampler\n",
        "class_sample_count = train_dataset.get_cls_num_list()\n",
        "weights = 1. / torch.Tensor(class_sample_count)\n",
        "samples_weight = torch.tensor([weights[t] for t in train_dataset.get_labels()])\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "trainloader_bal_pytorch = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, num_workers=4, sampler=sampler)\n",
        "for batch_idx, (inputs, targets) in enumerate(train_loader_bal):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    cls_batch_sampler = [(targets==i).sum().item() for i in range(10)]\n",
        "    print('Imbalanced Dataset Sampler with Pytorch:',cls_batch_sampler)\n",
        "    break\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Class frequency in Train Dataset [5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard IID Batch Sampler: [22, 9, 10, 2, 0, 4, 0, 2, 1, 0]\n",
            "Imbalanced Dataset Sampler with Lib: [8, 5, 5, 4, 6, 5, 7, 3, 4, 3]\n",
            "Imbalanced Dataset Sampler with Pytorch: [11, 3, 5, 3, 4, 5, 1, 6, 6, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRb9-UC653RU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}